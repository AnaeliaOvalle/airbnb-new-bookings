1: 5
0:10
-1:6
n = 26
n
0:n-1
0:(n-1)
a = 0:(n-1)
(365 - a) / 365
prod((365 - a) / 365)
1- prod((365 - a) / 365)
calc.prob
calc.prob(26)
calc.prob(26)
calc.prob <- function(n){
return (1-prod((365 - 0:(n-1))/365))
}
calc.prob(26)
make.plot<- function(nmax, ninv){
}
make.plot(100,26)
calc.prob <- function(n){
calc.prob(100,5)
calc.prob <- function(n){
calc.prob(5)
calc.prob(5)
dim(Hitters)
```{r}
library(ISLR)
? Hitters
summary(Hitters)
pdf(file = "plots.pdf")
```
There are some missing values here, so before we proceed we will remove them:
```{r}
Hitters=na.omit(Hitters)
?na.omit
with(Hitters,sum(is.na(Salary)))
```
Best Subset regression
------------------------
We will now use the package `leaps` to evaluate all the best-subset models.
```{r}
# install.packages("leaps")
?leaps
library(leaps)
?regsubsets
regfit.full=regsubsets(Salary~.,data=Hitters)
regfit.full
summary(regfit.full)
```
It gives by default best-subsets up to size 8; lets increase that to 19, i.e. all the variables
```{r}
regfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
```
There is a plot method for the `regsubsets`  object
```{r}
plot(regfit.full,scale="Cp")
coef(regfit.full,10)
```
Forward Stepwise Selection
--------------------------
Here we use the `regsubsets` function but specify the `method="forward" option:
```{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp")
```
Model Selection Using a Validation Set
---------------------------------------
Lets make a training and validation set, so that we can choose a good subset model.
We will do it using a slightly different approach from what was done in the the book.
```{r}
dim(Hitters)
set.seed(1)
#sampling our data without replacement
train=sample(seq(263),180,replace=FALSE)
#sampling our data without replacement 263 is n, number of observations. training = 180
train=sample(seq(263),180,replace=FALSE)
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19,method="forward")
val.errors=rep(NA,19)
#testing everything except for training
#we want to get a forward selection coefficient
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!
#for each of the models that we feed with 1 or 2 or...19 predictors
#we want to get a forward selection coefficient
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
#this is RSS
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type="b")
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
#this is RSS
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
#this is RSS
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type="b")
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
#this is RSS
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type="b")
points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b")
Model Selection
================
This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages,
and a very nice way of distributing an analysis. It has some very simple syntax rules.
```{r}
library(ISLR)
? Hitters
summary(Hitters)
pdf(file = "plots.pdf")
```
There are some missing values here, so before we proceed we will remove them:
```{r}
Hitters=na.omit(Hitters)
?na.omit
with(Hitters,sum(is.na(Salary)))
```
Best Subset regression
------------------------
We will now use the package `leaps` to evaluate all the best-subset models.
```{r}
# install.packages("leaps")
?leaps
library(leaps)
?regsubsets
regfit.full=regsubsets(Salary~.,data=Hitters)
regfit.full
summary(regfit.full)
```
It gives by default best-subsets up to size 8; lets increase that to 19, i.e. all the variables
```{r}
regfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
```
There is a plot method for the `regsubsets`  object
```{r}
plot(regfit.full,scale="Cp")
coef(regfit.full,10)
```
Forward Stepwise Selection
--------------------------
Here we use the `regsubsets` function but specify the `method="forward" option:
```{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp")
```
Model Selection Using a Validation Set
---------------------------------------
Lets make a training and validation set, so that we can choose a good subset model.
We will do it using a slightly different approach from what was done in the the book.
```{r}
dim(Hitters)
set.seed(1)
#sampling our data without replacement 263 is n, number of observations. training = 180
train=sample(seq(263),180,replace=FALSE)
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19,method="forward")
```
Now we will make predictions on the observations not used for training. We know there are 19 models, so we set up some vectors to record the errors. We have to do a bit of work here, because there is no predict method for `regsubsets`.
```{r}
val.errors=rep(NA,19)
#testing everything except for training
x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!
#for each of the models that we feed with 1 or 2 or...19 predictors
#we want to get a forward selection coefficient
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
#this is RSS
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type="b")
points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)
```
As we expect, the training error goes down monotonically as the model gets bigger, but not so
for the validation error.
This was a little tedious - not having a predict method for `regsubsets`. So we will write one!
```{r}
predict.regsubsets=function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form,newdata)
coefi=coef(object,id=id)
mat[,names(coefi)]%*%coefi
}
```
Model Selection by Cross-Validation
-----------------------------------
We will do 10-fold cross-validation. Its really easy!
```{r}
set.seed(11)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA,10,19)
for(k in 1:10){
best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
for(i in 1:19){
pred=predict(best.fit,Hitters[folds==k,],id=i)
cv.errors[k,i]=mean( (Hitters$Salary[folds==k]-pred)^2)
}
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19,type="b")
dev.off()
```
library(ISLR)
? Hitters
summary(Hitters)
pdf(file = "plots.pdf")
Hitters=na.omit(Hitters)
?na.omit
with(Hitters,sum(is.na(Salary)))
# install.packages("leaps")
?leaps
library(leaps)
?regsubsets
regfit.full=regsubsets(Salary~.,data=Hitters)
regfit.full
summary(regfit.full)
regfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
par(mfrow = c(1,1))
regfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
which.min(reg.summary$cp)
#import libraries
library(maps)
library(geosphere)
library(plyr)
library(beeswarm)
#set directory
setwd("~/Desktop/Data_Mining/airbnb-new-bookings")
#load in data
training= read.csv("train_users.csv", header = T)
testing = read.csv("test_users.csv", header = T)
#find features and dimenstions
names(training) #16 features - ID - country = 14 training predictors
dim(training)  #213,451 x 16
dim(testing)
library(maps)
library(geosphere)
library(plyr)
library(beeswarm)
#set directory
setwd("~/Desktop/Data_Mining/airbnb-new-bookings")
#load in data
training= read.csv("train_users.csv", header = T)
testing = read.csv("test_users.csv", header = T)
#find features and dimenstions
#import libraries
library(maps)
library(geosphere)
library(plyr)
library(beeswarm)
#set directory
setwd("~/Desktop/Data_Mining/airbnb-new-bookings")
getwd()
setwd("~/Desktop/Data_Mining/airbnb-new-bookings")
setwd("~/Desktop/Data_Mining/airbnb-new-bookings")
setwd("/~/Desktop/Data_Mining/airbnb-new-bookings")
setwd("/Users/eightiesfanjan/Desktop/Data_Mining/airbnb-new-bookings")
setwd("/airbnb-new-bookings")
setwd("airbnb-new-bookings")
#import libraries
library(maps)
library(geosphere)
library(plyr)
library(beeswarm)
library(corrplot)
library(Matrix)
library(caret)
library(randomForest)
library(MASS)
#set directory
setwd("~/Desktop/Data_Mining/Project/airbnb-new-bookings")
#load in data
testing = read.csv("test_users.csv", header = T, stringsAsFactors = F)
training= read.csv("train_users.csv", header = T, stringsAsFactors = F)
#find features and dimenstions
names(training) #16 features - ID - country = 14 training predictors
dim(training)  #213,451 x 16
dim(testing) #62,096 x 15
test_country_destination = rep("NA",nrow(testing))
#bind testing and training
#remember that testing is <6/30/2014
testing$country_destination = test_country_destination
dim(testing) #confirm
all_data = rbind(training, testing, make.row.names = T)
#convert time variable
all_data$timestamp_first_active = strptime(all_data$timestamp_first_active, format = "%Y%m%d%H%M%S")
all_data$timestamp_first_active = as.Date(all_data$timestamp_first_active)
all_data$date_account_created = as.Date(all_data$date_account)
class(all_data$timestamp_first_active)
as.data.frame(all_data$timestamp_first_active)
#summary before cleaning
summary(all_data)
#remove date_first_booking
all_data = all_data[, !(colnames(all_data) %in% "date_first_booking")]
# all_data = all_data[, -4]
#duplicate if needed
original_data = all_data
# all_data = original_data
# attach(all_data)
#cleaning age column for NA. Convert to -1
ages_count = unique(all_data$age)
ages_count
all_data$age[all_data$age>90 | all_data$age<15] = NA
ages_count = unique(all_data$age)
ages_count
sum(is.na(all_data$age))
all_data$age[is.na(all_data$age)] = -1
ages_count = unique(all_data$age)
ages_count
#clean first_affiliate_tracked and convert NA to -1
all_data$first_affiliate_tracked[all_data$first_affiliate_tracked == ""] = NA
affiliate_count= unique(all_data$first_affiliate_tracked)
affiliate_count
sum(is.na(all_data$first_affiliate_tracked))
all_data$first_affiliate_tracked[is.na(all_data$first_affiliate_tracked)] = -1
#summary after cleaning
summary(all_data)
#11 of 15 predictors are categorical, 2 are date, 2 are numerical
#check out percent of country destination
attach(all_data)
places = unique(country_destination)
places
freq = count(all_data$country_destination)
percent = round(freq[,2]/length(country_destination), 4)
response_count = data.frame(cbind(freq, percent))
response_count[order(response_count$percent),]
#exploration of numerical features
#age
x = all_data$age[all_data$age!=-1]
hist(x, freq = F, main = "Distribution of Age")
lines(density(x), col = "red", lty = "dotted", lwd = 1.5)
#date accound created
x = all_data$date_account_created
# barplot(prop.table(table(x)))
plot(count(all_data$date_account_created), main = "Airbnb Users from 2010-2014", xlab = "Years", ylab = "Frequency", pch = 18)
#gender
x = all_data$gender
barplot(prop.table(table(x)), col = "blue")
#data transformation!!
attach(all_data)
#one-hot encoding of signup_method
dummies = c(gender, signup_method, signup_flow, language, affiliate_channel, first_affiliate_tracked, affiliate_provider, signup_app, first_browser, first_device_type)
not_dummy = c(age,id, date_account_created, timestamp_first_active)
sub_data = subset(all_data, select = -c(age,id, date_account_created, timestamp_first_active))
# x = data.frame(model.matrix(~.-1, dd0), signup_method = all_data$signup_method)
# sparse_matrix = sparse.model.matrix(all_data$country_destination~.-1, data = all_data)
dummies = dummyVars(country_destination~., sub_data)
dummies
head(predict(dummies, newdata = all_data))
dummy_data = predict(dummies, newdata = all_data)
ncol(dummy_data)
as.data.frame(dummy_data)
summary(dummy_data)
#add 2 date columns and the target response
dummy_data = cbind(id, date_account_created, timestamp_first_active, age, dummy_data, country_destination)
#split into training and testing
clean_train_indices = 1:nrow(training)
clean_train = all_data[clean_train_indices,]
new_indices = round(0.7*nrow(clean_train))
new_train_indices = 1:new_indices
new_train = clean_train[new_train_indices,]
new_test_indices = (new_indices+1):(nrow(clean_train))
new_test_indices
length(new_test_indices)
new_test = clean_train[new_test_indices,]
nrow(new_test)
nrow(new_train)
categorical = c(signup_method,signup_flow, language, affiliate_channel, affiliate_provider, first_affiliate_tracked, signup_app,first_device_type,first_browser)
categorical = c(signup_method,signup_flow, language, affiliate_channel, affiliate_provider, first_affiliate_tracked, signup_app,first_device_type,first_browser)
categories = all_data[, colnames(all_data) %in% categorical]
rm(freq)
rm(Hitters)
categories
categories = new_train[, colnames(new_train) %in% categorical]
class(categories)
as.data.frame(categories)
categories
colnames(new_train)
categorical = c(signup_method,signup_flow, language, affiliate_channel, affiliate_provider, first_affiliate_tracked, signup_app,first_device_type,first_browser)
colnames(new_train) %in% categorical
categorical = c("signup_method","signup_flow", "language", "affiliate_channel", "affiliate_provider", "first_affiliate_tracked", "signup_app","first_device_type","first_browser")
categories = new_train[, colnames(new_train) %in% categorical]
A = model.matrix(country_destination ~ categorical)
class(categories)
categories = c("signup_method","signup_flow", "language", "affiliate_channel", "affiliate_provider", "first_affiliate_tracked", "signup_app","first_device_type","first_browser")
categorical = new_train[, colnames(new_train) %in% categories]
A = model.matrix(country_destination ~ categorical)
categories = c("signup_method","signup_flow", "language", "affiliate_channel", "affiliate_provider", "first_affiliate_tracked", "signup_app","first_device_type","first_browser")
categorical = new_train[, colnames(new_train) %in% categories]
A = model.matrix(country_destination ~ categorical)
A = model.matrix(country_destination ~ colnames(categorical))
A = model.matrix(country_destination ~ colname(categorical))
A = model.matrix(country_destination ~ categories)
A = model.matrix(country_destination ~ .)
A = model.matrix(country_destination ~ ., data = categorical)
View(categorical)
dim(categorical)
rnow(categorical$signup_method)
nrow(categorical$signup_method)
length(categorical$signup_method)
any.na(categorical$signup_method)
A = model.matrix(~ categorical)
categories = c("country_destination", "signup_method","signup_flow", "language", "affiliate_channel", "affiliate_provider", "first_affiliate_tracked", "signup_app","first_device_type","first_browser")
categorical = new_train[, colnames(new_train) %in% categories]
categories = c("country_destination", "signup_method","signup_flow", "language", "affiliate_channel", "affiliate_provider", "first_affiliate_tracked", "signup_app","first_device_type","first_browser")
categorical = new_train[, colnames(new_train) %in% categories]
length(categorical$signup_method)
attach(categorical)
A = model.matrix(country_destination~ categorical)
class(categorical)
A = model.matrix(country_destination~ categories
A = model.matrix(country_destination~ categories)
A = model.matrix(country_destination~ ., categorical)
View(A)
dim(A)
dummy_data = model.matrix(country_destination~ ., categorical)
with_dummy_train= cbind(new_train$country_destination,dummy_data)
View(with_dummy_train)
contrast()?
?contrast
?contrasts
as.factor(new_train$signup_flow)
contrasts(new_train$signup_flow)
new_train$signup_flow=as.factor(new_train$signup_flow)
contrasts(new_train$signup_flow)
View(new_train)
new_train$signup_flow=as.factor(new_train$signup_flow)
#use model.matrix to convert
categories = c("country_destination", "signup_method","signup_flow", "language", "affiliate_channel", "affiliate_provider", "first_affiliate_tracked", "signup_app","first_device_type","first_browser")
categorical = new_train[, colnames(new_train) %in% categories]
length(categorical$signup_method)
dummy_data = model.matrix(country_destination~ ., categorical)
with_dummy_train= cbind(new_train$country_destination,dummy_data)
View(with_dummy_train)
categories = c("country_destination", "signup_method","signup_flow", "language", "affiliate_channel", "affiliate_provider", "first_affiliate_tracked", "signup_app","first_device_type","first_browser")
categorical = new_train[, colnames(new_train) %in% categories]
length(categorical$signup_method)
dummy_data = model.matrix(country_destination-1~ ., categorical)
dummy_data = model.matrix(country_destination~ .-1, categorical)
with_dummy_train= cbind(new_train$country_destination,dummy_data)
View(with_dummy_train)
View(dummy_data)
dummy_data = model.matrix(country_destination~ ., categorical)
with_dummy_train= cbind(new_train$country_destination,dummy_data)
dummy_data = model.matrix(country_destination~ .-1, categorical)
with_dummy_train= cbind(new_train$country_destination,dummy_data)
View(with_dummy_train)
colnames(with_dummy_train)
colnames(with_dummy_train)[1] = "country_destination"
View(with_dummy_train)
#######QDA
qda.fit = qda(country_destination~., data = with_dummy_train, subset = new_train_indices)
type(with_dummy_train)
class(with_dummy_train)
as.data.frame(with_dummy_train)
View(with_dummy_train)
qda.fit = qda(country_destination~., data = with_dummy_train, subset = new_train_indices)
with_dummy_train = as.data.frame(with_dummy_train)
#######QDA
qda.fit = qda(country_destination~., data = with_dummy_train, subset = new_train_indices)
attach(with_dummy_train)
unattach(all_data)
